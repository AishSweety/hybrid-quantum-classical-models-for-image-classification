{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CNN4"
      ],
      "metadata": {
        "id": "b56c8rM5cN-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "-RM98MtEeTww"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading of the MNIST dataset\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',  # root: The root directory where the dataset should be stored. In this case, it is set to 'data'. If the 'data' directory doesn't exist, the dataset will be downloaded to this location.\n",
        "    train = True,\n",
        "    transform = transforms.ToTensor(),  # transform: This parameter applies transformations to the data. In this case, transforms.ToTensor() is used to convert the images to PyTorch tensors.\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = transforms.ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "_I845zHUeoXV"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting of the main hyper-parameters of the model\n",
        "batch_size = 4 # The number of samples in each mini-batch used during training. Smaller batch sizes can lead to faster convergence but may introduce more noise into the training process.\n",
        "n_train = batch_size * 125    # The total size of the training dataset. It's calculated as the product of batch_size and the number of batches (125 in this case). Adjusting the training dataset size can impact the model's ability to generalize.\n",
        "n_test = batch_size * 25     # The total size of the test dataset. Similar to n_train, it's calculated as the product of batch_size and the number of test batches (25 in this case). The test dataset is used to evaluate the model's performance on unseen data.\n",
        "n_channels = 4  # The number of channels in the output of the quantum convolution layer. In your model, you have set it to 4. This parameter determines the depth of the feature maps produced by the convolutional layer.\n",
        "initial_lr =  0.005     # The initial learning rate for the stochastic gradient descent (SGD) optimizer."
      ],
      "metadata": {
        "id": "yFlOU3zGecUq"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layer 1 with 1 input channels, 4 output channels, and 4x4 kernel\n",
        "        self.conv = nn.Conv2d(1, 4, 4, stride=4)\n",
        "        self.fc = nn.Linear(4 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagate the input through the CNN layers\n",
        "        x = self.conv(x)\n",
        "        # Flatten the output from the convolutional layer\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc(x))\n",
        "        return x\n",
        "cnn=Net()"
      ],
      "metadata": {
        "id": "-3CmM5H-BOh5"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset  = train_data\n",
        "train_size = n_train\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "for data in train_loader:\n",
        "    inputs, labels = data\n",
        "    print(f\"{inputs.shape=}\")\n",
        "    print(f\"{labels=}\")\n",
        "    outputs = cnn(inputs)\n",
        "    print(f\"{outputs.shape=}\")\n",
        "    print(f\"{outputs=}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlHwR2hdRzb2",
        "outputId": "d4510d66-8f4f-4807-f13f-30d9c0402f10"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape=torch.Size([4, 1, 28, 28])\n",
            "labels=tensor([4, 0, 9, 2])\n",
            "outputs.shape=torch.Size([4, 10])\n",
            "outputs=tensor([[0.0000, 0.2931, 0.0000, 0.0509, 0.1659, 0.0686, 0.0000, 0.0630, 0.0587,\n",
            "         0.2090],\n",
            "        [0.0681, 0.1966, 0.0000, 0.0923, 0.0608, 0.0000, 0.0000, 0.0133, 0.0989,\n",
            "         0.0934],\n",
            "        [0.0000, 0.2260, 0.0000, 0.0000, 0.1664, 0.0000, 0.0000, 0.1725, 0.0000,\n",
            "         0.4029],\n",
            "        [0.0000, 0.2944, 0.0000, 0.0733, 0.1128, 0.0000, 0.0000, 0.0473, 0.0000,\n",
            "         0.1634]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "dataset  = train_data\n",
        "\n",
        "# Initialize your QCNN model\n",
        "cnn = Net()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.SGD(cnn.parameters(), lr=initial_lr, momentum=0.90)  # Stochastic Gradient Descent optimizer\n",
        "# Create a learning rate scheduler\n",
        "# Here, we use StepLR which reduces the learning rate by a factor every step_size epochs\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.0)\n",
        "# Split your data into training and validation sets\n",
        "train_size = n_train\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"ImgClass-Quanvolv.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "RESUME_TRAINING = True\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "loss_list = []\n",
        "cnn.train()\n",
        "\n",
        "if RESUME_TRAINING is False:\n",
        "    print(f\"Restore model state from {MODEL_SAVE_PATH}\")\n",
        "    if os.path.exists(MODEL_SAVE_PATH):\n",
        "        model_dict = torch.load(MODEL_SAVE_PATH)\n",
        "        initial_epoch = model_dict['epoch'] + 1\n",
        "        cnn.load_state_dict(model_dict['model_state_dict'])\n",
        "        optimizer.load_state_dict(model_dict['optimizer_state_dict'])\n",
        "        loss_list = model_dict['loss'].copy()\n",
        "    else:\n",
        "        print(f\"No saved model state found. Training from scratch.\")\n",
        "        initial_epoch = 0\n",
        "        loss_list = []\n",
        "else:\n",
        "    initial_epoch = 0\n",
        "    loss_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ct = datetime.datetime.now()\n",
        "    # Decay Learning Rate\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    lr = scheduler.get_last_lr()\n",
        "    print(f\"{epoch=}, {lr=}, {ct}\")\n",
        "    running_loss = []\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients to avoid accumulation\n",
        "        outputs = cnn(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        running_loss.append(loss.item())\n",
        "        optimizer.step()  # Update the model parameters\n",
        "    loss_list.append(sum(running_loss) / len(running_loss))\n",
        "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(100. * (epoch + 1) / num_epochs, loss_list[-1]))\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': cnn.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss_list,\n",
        "    }, MODEL_SAVE_PATH)\n",
        "    print(f\"Saving model state to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "RCxAXutdvoRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4668e779-7c7e-4410-9b25-695de58a33b8"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, lr=[0.005], 2024-01-06 04:49:22.671176\n",
            "Training [5%]\tLoss: 2.2268\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=1, lr=[0.005], 2024-01-06 04:49:22.895774\n",
            "Training [10%]\tLoss: 1.6212\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=2, lr=[0.005], 2024-01-06 04:49:23.117166\n",
            "Training [15%]\tLoss: 1.2250\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=3, lr=[0.005], 2024-01-06 04:49:23.357329\n",
            "Training [20%]\tLoss: 1.1190\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=4, lr=[0.005], 2024-01-06 04:49:23.570795\n",
            "Training [25%]\tLoss: 1.0462\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=5, lr=[0.005], 2024-01-06 04:49:23.784763\n",
            "Training [30%]\tLoss: 1.0016\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=6, lr=[0.005], 2024-01-06 04:49:23.999354\n",
            "Training [35%]\tLoss: 0.9918\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=7, lr=[0.005], 2024-01-06 04:49:24.208826\n",
            "Training [40%]\tLoss: 0.9384\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=8, lr=[0.005], 2024-01-06 04:49:24.427452\n",
            "Training [45%]\tLoss: 0.9241\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=9, lr=[0.005], 2024-01-06 04:49:24.643629\n",
            "Training [50%]\tLoss: 0.8962\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=10, lr=[0.005], 2024-01-06 04:49:24.859728\n",
            "Training [55%]\tLoss: 0.8939\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=11, lr=[0.005], 2024-01-06 04:49:25.069977\n",
            "Training [60%]\tLoss: 0.8646\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=12, lr=[0.005], 2024-01-06 04:49:25.273394\n",
            "Training [65%]\tLoss: 0.8554\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=13, lr=[0.005], 2024-01-06 04:49:25.497585\n",
            "Training [70%]\tLoss: 0.8444\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=14, lr=[0.005], 2024-01-06 04:49:25.707545\n",
            "Training [75%]\tLoss: 0.8379\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=15, lr=[0.005], 2024-01-06 04:49:25.921570\n",
            "Training [80%]\tLoss: 0.8275\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=16, lr=[0.005], 2024-01-06 04:49:26.133493\n",
            "Training [85%]\tLoss: 0.8217\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=17, lr=[0.005], 2024-01-06 04:49:26.350431\n",
            "Training [90%]\tLoss: 0.8122\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=18, lr=[0.005], 2024-01-06 04:49:26.589532\n",
            "Training [95%]\tLoss: 0.8084\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "epoch=19, lr=[0.005], 2024-01-06 04:49:26.886995\n",
            "Training [100%]\tLoss: 0.8084\n",
            "Saving model state to models/ImgClass-Quanvolv.pth\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.state_dict()"
      ],
      "metadata": {
        "id": "nUl3J1rhtfvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be827968-f89b-4514-8e5a-d9efb0532f0a"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('conv.weight',\n",
              "              tensor([[[[-9.2388e-01, -5.8929e-01, -6.7031e-01, -5.2077e-01],\n",
              "                        [-1.0605e+00, -1.0203e+00, -8.6686e-01, -3.9097e-01],\n",
              "                        [-1.0201e+00, -6.6626e-01, -2.3081e-01, -3.9852e-01],\n",
              "                        [-8.1234e-01, -2.5749e-01,  8.2552e-04,  2.5906e-02]]],\n",
              "              \n",
              "              \n",
              "                      [[[-1.7290e-01, -1.0526e-01,  2.5084e-02,  5.1990e-01],\n",
              "                        [ 4.0717e-02, -6.1931e-02,  2.3308e-01,  1.1402e-01],\n",
              "                        [-1.9381e-01, -2.5186e-01, -1.3225e-01, -7.5126e-01],\n",
              "                        [-5.5205e-01, -6.6428e-01, -2.5852e-01, -9.1830e-01]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 1.7420e-01,  4.1984e-01,  9.8764e-01,  5.3831e-01],\n",
              "                        [-1.9157e-02, -6.4329e-02,  3.8352e-01,  5.9392e-01],\n",
              "                        [ 6.6614e-02,  1.3925e-01,  8.1434e-01,  1.3035e+00],\n",
              "                        [-4.7035e-02,  5.9577e-01,  1.0783e+00,  1.1108e+00]]],\n",
              "              \n",
              "              \n",
              "                      [[[ 8.2520e-01,  6.2562e-01,  1.9157e-01, -2.6445e-01],\n",
              "                        [ 6.4359e-01,  5.0645e-01, -1.4111e-01, -4.5830e-01],\n",
              "                        [ 2.8926e-01, -2.6936e-02, -5.6172e-01, -6.2375e-01],\n",
              "                        [-4.8164e-01, -2.0072e-01, -6.4104e-01, -8.8802e-01]]]])),\n",
              "             ('conv.bias', tensor([ 0.6154,  0.2659, -0.6529, -0.0022])),\n",
              "             ('fc.weight',\n",
              "              tensor([[-0.0908, -0.0592,  0.0221,  ..., -0.0667,  0.0262, -0.0283],\n",
              "                      [-0.0855, -0.0359, -0.0147,  ...,  0.0358,  0.0183, -0.0358],\n",
              "                      [ 0.0523,  0.0282, -0.0601,  ..., -0.1361, -0.0186, -0.0759],\n",
              "                      ...,\n",
              "                      [-0.0512,  0.0172,  0.0214,  ...,  0.0456,  0.0002, -0.0607],\n",
              "                      [-0.0275, -0.0169,  0.0243,  ..., -0.0080, -0.0436, -0.0173],\n",
              "                      [ 0.0692, -0.0282,  0.0634,  ...,  0.0649, -0.0513,  0.0403]])),\n",
              "             ('fc.bias',\n",
              "              tensor([-0.1888,  0.0599,  0.1772, -0.0362, -0.0030,  0.0218, -0.0206,  0.1077,\n",
              "                      -0.0287, -0.0747]))])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "\n",
        "# Use a small subset of the full validation dataset\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "K = n_test # enter your length here\n",
        "subsample_train_indices = torch.randperm(len(val_set))[:K]\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, sampler=SubsetRandomSampler(subsample_train_indices))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# Set the model to evaluation mode\n",
        "cnn.eval()\n",
        "with torch.inference_mode():\n",
        "    for data in val_loader:\n",
        "        images, labels = data\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'Accuracy on the validation set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "EZCJdLCoqc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25720c9e-cc37-484b-ccbf-fe25eba068c6"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the validation set: 67.00%\n"
          ]
        }
      ]
    }
  ]
}